# GRPO from scratch 

Implementations of **Group Relative Policy Optimization (GRPO)** in PyTorch, built from first principles—no reliance on libraries like [TRL](https://github.com/huggingface/trl) or [VERL](https://github.com/volcengine/verl). 

- GRPO paper: [arXiv:2402.03300](https://arxiv.org/abs/2402.03300)

## Motivation

this is for learning purposes and to make it less of a black box

i want to demystify the training stack—masking, KL control, scheduling, and evaluation—so you can see exactly how these methods work end to end.

Although libs like TRL, VERL, Puffer are great, i want to understand the internals of these methods and how they work.

## 

---
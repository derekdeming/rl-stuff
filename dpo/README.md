# DPO from scratch 

This repo implements **Direct Preference Optimization (DPO)** from scratch in PyTorch, without relying on off-the-shelf libraries like [TRL](https://github.com/huggingface/trl) or [VERL](https://github.com/volcengine/verl).

- DPO paper: [arXiv:2305.18290](https://arxiv.org/abs/2305.18290)

## Motivation

this is for learning purposes and to make it less of a black box
i want to demystify the training stack—masking, KL control, scheduling, and evaluation—so you can see exactly how these methods work end to end.

Although libs like TRL, VERL, Puffer are great, i want to understand the internals of these methods and how they work. I am going to also implement GRPO from scratch.



## 

---